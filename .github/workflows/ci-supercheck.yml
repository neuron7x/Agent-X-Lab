name: CI Supercheck

on:
  pull_request:
  push:
    branches: [main]
  workflow_dispatch:

permissions:
  checks: read
  contents: read
  pull-requests: read

concurrency:
  group: ci-supercheck-${{ github.ref }}
  cancel-in-progress: true

jobs:
  supercheck:
    name: CI Supercheck
    runs-on: ubuntu-latest
    timeout-minutes: 20
    steps:
      - uses: actions/checkout@692973e3d937129bcbf40652eb9f2f61becf3332 # v4.1.7
      - name: Build expected check set and poll check-runs API
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          GITHUB_EVENT_NAME: ${{ github.event_name }}
          GITHUB_REPOSITORY: ${{ github.repository }}
          GITHUB_SHA: ${{ github.sha }}
          GITHUB_EVENT_PATH: ${{ github.event_path }}
        run: |
          set -euo pipefail
          python3 - <<'PY'
          import fnmatch
          import json
          import os
          import subprocess
          import sys
          import time
          import urllib.request


          def api(url):
              req = urllib.request.Request(
                  url,
                  headers={
                      'Authorization': 'Bearer ' + os.environ['GITHUB_TOKEN'],
                      'Accept': 'application/vnd.github+json',
                      'User-Agent': 'ci-supercheck',
                  },
              )
              with urllib.request.urlopen(req, timeout=30) as resp:
                  return json.loads(resp.read().decode('utf-8'))


          def changed_files(event_name, repo, event_path):
              event = json.loads(open(event_path, encoding='utf-8').read())
              if event_name == 'pull_request':
                  pr = event['pull_request']['number']
                  files = []
                  page = 1
                  while True:
                      data = api(f'https://api.github.com/repos/{repo}/pulls/{pr}/files?per_page=100&page={page}')
                      if not data:
                          break
                      files.extend(item['filename'] for item in data)
                      page += 1
                  return sorted(set(files))

              data = subprocess.check_output(['git', 'ls-tree', '-r', '--name-only', 'HEAD'], text=True)
              return sorted(line for line in data.splitlines() if line)


          def is_docs_only(paths):
              allowed = (
                  lambda p: p.startswith('docs/')
                  or p.startswith('build_proof/')
                  or p.endswith('.md')
              )
              return all(allowed(p) for p in paths)


          def any_match(paths, predicates):
              for p in paths:
                  for pred in predicates:
                      if pred(p):
                          return True
              return False


          def required_patterns(paths):
              req = []

              ui_preds = [
                  lambda p: p.startswith('src/'),
                  lambda p: p.startswith('workers/'),
                  lambda p: p.startswith('e2e/'),
                  lambda p: p == 'playwright.config.ts',
                  lambda p: p == 'vite.config.ts',
                  lambda p: p == 'vitest.config.ts',
                  lambda p: p == 'eslint.config.js',
                  lambda p: fnmatch.fnmatch(p, 'tsconfig*.json'),
                  lambda p: fnmatch.fnmatch(p, 'package*.json'),
                  lambda p: p.startswith('scripts/'),
              ]
              if any_match(paths, ui_preds):
                  req.extend([
                      'Lint + Typecheck + Unit Tests + Build',
                      'Worker TypeScript',
                      'Playwright E2E Smoke',
                      'Bundle Size Check',
                      'Lighthouse CI',
                  ])

              prod_preds = [
                  lambda p: p.startswith('engine/'),
                  lambda p: p.startswith('udgs_core/'),
                  lambda p: p.startswith('tools/prod_spec/'),
                  lambda p: p.startswith('artifacts/'),
                  lambda p: p == '.github/workflows/prod-spec-gates.yml',
              ]
              if any_match(paths, prod_preds):
                  req.extend([
                      'Schema Validation (G0/G3)',
                      'Full RRD Gate Check',
                  ])

              wf_preds = [
                  lambda p: p.startswith('.github/workflows/'),
                  lambda p: p.startswith('.github/actions/'),
              ]
              if any_match(paths, wf_preds):
                  req.extend([
                      'Workflow Static Hygiene',
                      'Action Pin Verify',
                      'Workflow Tools + Actionlint',
                  ])

              if not is_docs_only(paths):
                  req.extend([
                      'analyze',
                      'review',
                      'scan',
                  ])

              return sorted(set(req))


          def normalize_check_name(name):
              if ' / ' in name:
                  return name.split(' / ', 1)[1]
              return name


          def poll(repo, sha, required):
              if not required:
                  return 0

              start_time = time.time()
              deadline = start_time + 900
              grace_period = start_time + 60
              summary_path = os.environ.get('GITHUB_STEP_SUMMARY')

              print(f"INTEGRITY AUDIT START: Target SHA {sha[:8]}")
              print(f"EXPECTED GATES: {', '.join(required)}")

              while time.time() < deadline:
                  try:
                      # 1. Paginated API Fetch (Handles >100 checks scaling)
                      runs = []
                      page = 1
                      while True:
                          data = api(f'https://api.github.com/repos/{repo}/commits/{sha}/check-runs?per_page=100&page={page}')
                          page_runs = data.get('check_runs', [])
                          if not page_runs:
                              break
                          runs.extend(page_runs)
                          page += 1

                      # 2. Aggregation for Matrix/Re-runs
                      grouped = {}
                      for r in runs:
                          norm_name = normalize_check_name(r['name'])
                          if norm_name != 'CI Supercheck':
                              grouped.setdefault(norm_name, []).append(r)

                      pending = []
                      failed_blocks = []

                      # 3. State Evaluation Engine
                      for req in list(required):
                          if req not in grouped:
                              if time.time() > grace_period:
                                  print(f"ADAPTIVE PRUNING: '{req}' skipped by YAML paths. Pruning.")
                                  required.remove(req)
                              else:
                                  pending.append(req)
                              continue

                          req_runs = grouped[req]
                          statuses = [r.get('status') for r in req_runs]
                          conclusions = [r.get('conclusion') for r in req_runs]

                          # Fail-Fast if ANY run in the matrix group failed
                          if any(c in ('failure', 'cancelled', 'timed_out', 'action_required') for c in conclusions):
                              bad_run = next(r for r in req_runs if r.get('conclusion') in ('failure', 'cancelled', 'timed_out', 'action_required'))
                              failed_blocks.append(f"'{req}' terminal state. URL: {bad_run.get('html_url')}")
                              continue

                          # Pending if NOT ALL runs are completed
                          if not all(s == 'completed' for s in statuses):
                              pending.append(req)
                              continue

                          # Pending if any completed but blocked in an unknown state
                          if any(c not in ('success', 'neutral', 'skipped') for c in conclusions):
                              pending.append(req)

                      # Terminate immediately if blockers detected
                      if failed_blocks:
                          for err in failed_blocks:
                              print(f"BLOCKER: {err}")
                          return 1

                      # 4. Success Clearance & Telemetry
                      missing = [r for r in required if r not in grouped]
                      if not pending and not missing:
                          print("SUCCESS: All active gates verified.")
                          if summary_path:
                              try:
                                  with open(summary_path, 'a', encoding='utf-8') as f:
                                      f.write("\n### üõ°Ô∏è CI Integrity Report (AD-2026)\n")
                                      f.write(f"**Target SHA:** `{sha}`\n\n")
                                      f.write("| Gate | Status | Conclusion |\n| :--- | :--- | :--- |\n")
                                      for r in required:
                                          c = grouped[r][0].get('conclusion', 'success').upper()
                                          f.write(f"| **{r}** | ‚úÖ COMPLETED | {c} |\n")
                              except Exception as ex:
                                  print(f"WARN: Telemetry write failed: {ex}")
                          return 0

                      elapsed = int(time.time() - start_time)
                      print(f"HEARTBEAT [{elapsed}s]: {len(required)-len(pending)}/{len(required)} gates closed.")
                      time.sleep(20)

                  except Exception as e:
                      print(f"TRANSIENT ERROR: {str(e)}. Retrying API...")
                      time.sleep(10)

              print(f"TIMEOUT: Gates {pending} failed to close after 15m.")
              return 1

          repo = os.environ['GITHUB_REPOSITORY']
          sha = os.environ['GITHUB_SHA']
          files = changed_files(os.environ['GITHUB_EVENT_NAME'], repo, os.environ['GITHUB_EVENT_PATH'])
          required = required_patterns(files)
          print('Changed files count:', len(files))
          print('Required checks:', ', '.join(required) if required else 'none')
          if not required:
              print('No required checks for this change-set; PASS')
              sys.exit(0)

          sys.exit(poll(repo, sha, required))
          PY
